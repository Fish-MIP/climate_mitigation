---
title: "Extract data for climate mitigation project"
author: "Julia Blanchard, Camilla Novaglio, Derek Tittensor"
date: "2023-05-18"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Aim

This script is for extracting FishMIP global marine ecosystem model outputs for use in a cross-sectoral climate mitigation paper being led by Christoph Semken.

For this project we need th global time series of biomass outputs, spanning size ranges typical of exploited fishes - 10g to 1000 kg.

We will calculate annual time series spanning 1850-2100 under the rcp8.5 and rcp2.6 scenarios for each of the models, for whihc outputs are available:

APECOSM  - available, IPSL only
BOATS  - available
DBEM  - possible later to request OR possible using Lmax classes?
DBPM - available
EcoOcean  - possible using Lmax classes?
EcoTroph  - available?
FEISTY  - possible using Lmax classes?
MACROECOLOGICAL  - available
ZooMSS - available

## File dowload 

check FishMIP/EmergentConstraints repo R/01_extract_modelled_data.Rmd for details and notes. 

## Set environment 

```{r cars}

rm(list=ls())

### Read in files and extract modelled data at global scale
library(raster)
library(ncdf4)
library(tidyverse)
library(ncdf4.helpers)
library(parallel)
library(pbapply)
library(dplyr, warn.conflicts = FALSE)

```

## Read in files and extract modelled data at global scale

```{r}

### using files and code prepared by Cami:

dir<-"/rd/gem/private/users/camillan/EmergentConstraintData"

mem<-c("apecosm","boats", "dbpm", "zoomss", "ecotroph", "macroecological") 
esm<-c("gfdl-esm4", "ipsl-cm6a-lr")
scenario<-c("historical", "ssp126", "ssp585") 

# all

combinations<-expand.grid(mem = mem, esm = esm, scenario = scenario) %>%
  mutate(resolution = ifelse(mem %in% c("zoomss", "ecotroph", "macroecological"), "annual", "monthly"), 
         year = case_when(
           mem %in% c("boats", "zoomss", "ecotroph", "macroecological") & scenario == "historical" ~ "1950_2014", 
           mem %in% c("apecosm", "dbpm") & scenario == "historical" ~ "1850_2014",
           scenario %in% c("ssp126", "ssp585") ~ "2015_2100"), 
         mem = as.character(mem), 
         esm = as.character(esm), 
         scenario = as.character(scenario),
         netcdf_name = paste0(paste(mem, esm, "nobasd", scenario, "nat_default_tcblog10_global", resolution, year, sep ="_"), ".nc"),
         esm_simpler = ifelse(esm == "ipsl-cm6a-lr", "ipsl","gfdl"),
         identifier = paste(mem, esm_simpler, scenario, sep ="_")) %>% 
  select(-esm_simpler) %>% 
  arrange(mem, esm, scenario)

# apply function in // ----

netcdf = combinations$netcdf_name

# function to be applied
extract_global_outputs<-function(netcdf, file = "new"){
  
  # # trial
  # netcdf = "ecotroph_gfdl-esm4_nobasd_historical_nat_default_tcblog10_global_annual_1950_2014.nc"
  # netcdf = "apecosm_ipsl-cm6a-lr_nobasd_historical_nat_default_tcblog10_global_monthly_1850_2014.nc"
  # netcdf = "macroecological_ipsl-cm6a-lr_nobasd_ssp585_nat_default_tcblog10_global_annual_2015_2100.nc"
  # file = "new"

  if(file.exists(file.path(dir, netcdf))){
  
  ######### extract info from netcdf name and print warnings ----
  model = sub("\\_.*", "", netcdf)
  
  if(str_detect(netcdf, "gfdl", negate = FALSE)){
    esm = "gfdl-esm4"
  }else if (str_detect(netcdf, "ipsl", negate = FALSE)){
    esm = "ipsl-cm6a-lr"
  }
  
  # WARNING - add in EC? 
  if(str_detect(netcdf, "monthly", negate = FALSE)){
    time_step = "monthly"
  }else if (str_detect(netcdf, "annual", negate = FALSE)){
    time_step = "annual"
  }
  
  if(str_detect(netcdf, "historical", negate = FALSE)){
    scenario = "historical"
  }else if (str_detect(netcdf, "ssp126", negate = FALSE)){
    scenario = "ssp1"
  }else if (str_detect(netcdf, "ssp585", negate = FALSE)){
    scenario = "ssp5"
  }else if (str_detect(netcdf, "picontrol|2100", negate = FALSE)) {
    scenario = "picontrol_fut"
  } else if (str_detect(netcdf, "picontrol|2014", negate = FALSE)) {
    scenario = "picontrol_hist"}
  
  # extract info from netcdf description: 
  nc_data <- nc_open(file.path(dir, netcdf))
  
  lon <- ncvar_get(nc_data, "lon")
  lat <- ncvar_get(nc_data, "lat", verbose = F)
  t <- as.character(nc.get.time.series(nc_data))
  
  # this is only to FIX zoom size bins names 
  if(model != "zoomss" & file == "new"){
    bins<-ncvar_get(nc_data, "bins")
  }else if (model != "zoomss" & file == "old"){ # this is only to check DBPM old files (not in DKRZ)
    bins<-ncvar_get(nc_data, "size")
  }else if (model == "zoomss"){
    bins<-c(1:6)}
  
  t_units<-ncatt_get(nc_data, "time", "units")$value
  b_units<-ncatt_get(nc_data, "tcblog10", "units")$value
  
  nc_close(nc_data)
  
  # print warnings 
  
  stLon<-lon[1]
  enLon<-lon[length(lon)]
  stLat<-lat[1]
  enLat<-lat[length(lat)]
  stTime<-t[1]
  enTime<-t[length(t)]
  
  if(stLon != -179.5){
    warning(paste(model, esm, scenario, "incorrect starting Lon", sep = " "), immediate. = TRUE)
  }
  if(enLon != 179.5){
    warning(paste(model, esm, scenario, "incorrect ending Lon", sep = " "), immediate. = TRUE)
  }
  if(stLat != 89.5){
    warning(paste(model, esm, scenario, "incorrect starting Lat", sep = " "), immediate. = TRUE)
  }
  if(enLat != -89.5){
    warning(paste(model, esm, scenario, "incorrect ending Lat", sep = " "), immediate. = TRUE)
  }
  if(scenario == "historical" & !stTime %in% c("1950-01-01","1850-01-01")){ # some model include 100 years more 
    warning(paste(model, esm, scenario, "incorrect starting time", sep = " "), immediate. = TRUE)
  }
  if(scenario != "historical" & !stTime %in% c("2015-01-01")){
    warning(paste(model, esm, scenario, "incorrect starting time", sep = " "), immediate. = TRUE)
  }
  if(scenario == "historical" & !enTime %in% c("2014-12-01", "2014-01-01")){ # models can be monthly or annual 
    warning(paste(model, esm, scenario, "incorrect ending time", sep = " "), immediate. = TRUE)
  }
  if(scenario != "historical" & !enTime %in% c("2100-12-01", "2100-01-01")){ # models can be monthly or annual 
    warning(paste(model, esm, scenario, "incorrect ending time", sep = " "), immediate. = TRUE)
  }
  if(t_units != "days since 1601-1-1 00:00:00"){
    warning(paste(model, esm, scenario, "incorrect time units", sep = " "), immediate. = TRUE)
  }
  if(bins[1] != 1 & bins[6] != 6){
    warning(paste(model, esm, scenario, "incorrect bins names", sep = " "), immediate. = TRUE)
  }
  if(bins[length(bins)] != 6){
    warning(paste(model, esm, scenario, "incorrect bins dimension", sep = " "), immediate. = TRUE)
  }
  if(b_units != "g m-2"){
    warning(paste(model, esm, scenario, "incorrect biomass units", sep = " "), immediate. = TRUE)
  }
 
  # extract data as raster object: 
  brick_data<-list()
 
  for (i in 1:length(bins)){
    brick_data[[i]]<-brick(file.path(dir, netcdf), level = i)
    print(dim(brick_data[[i]]))
  }
  
  #### IF JB and DT approach - function end here and output is:
  # return(brick_data)

  ######### WARINING - remove marginal seas using the land-sea IPSL mask provided by Matthias TO DO ----

  ######### calculate total annual sums (OR weighted annual means) ----

  # STEP 1 - remove 1850-1950 as not all models have them
  indices<-t

  if(scenario %in% c("historical","picontrol_hist")){
    indices_subset<-indices[indices>="1950-01-01"]
    indices_position<-match(indices_subset,indices)
    brick_data_subset<-lapply(brick_data, FUN = function(x) raster::subset(x, indices_position))
  }else if (scenario %in% c("ssp1","ssp5","picontrol_fut")){
    brick_data_subset<-brick_data
    } else if(scenario == "picontrol_whole"){
    indices_subset<-indices[indices>="2015-01-01"]
    indices_position<-match(indices_subset,indices)
    brick_data_subset<-lapply(brick_data, FUN = function(x) raster::subset(x, indices_position))
  }

  # STEP 2 - calculate annual sums: 
  # Julia: multiply each grid cell value by cell area and sum over all grid cells.
  # to multiply by area, need to convert area from km2 to m2 (*1e6), and sum over all cells each month. # from area() If x is a Raster* object: RasterLayer or RasterBrick. Cell values represent the size of the cell in km2
  
  # create vector to serve as index

  if(scenario %in% c("historical","picontrol_hist", "picontrol_whole")){
    indices2<-as.Date(indices_subset)
  }else if(scenario %in% c("ssp1", "ssp5", "picontrol_fut")){
    indices2<-as.Date(t)}

  indices2<-format(indices2, format = "%Y")
  indices2<-as.numeric(indices2)
  
  # WARNING whether it is monthly or annual outputs? i.e. sum across months and then across lat/long below? note that biomass is a stock not a rate and does not have time dimension 
  brick_data_annual<-lapply(brick_data_subset, FUN = function(x) stackApply(x, indices=indices2, fun=sum))

  # # CHECK
  # dim(brick_data_annual[[1]])
  # plot(brick_data_annual[[1]][[15]])
  # plot(brick_data_subset[[1]][[15]])

  if(scenario == "picontrol_whole"){scenario = "picontrol_fut"}
  
  # create a raster with latitude cell values (outside loop for efficiency)
  w <- area(brick_data_annual[[1]])*1e6 # WARNING - the code below does /10000, why?? adjust in EC even if not used, and in FishingEffort when calculating trends in climate inputs (code 09, again not used but should be corrected) 
  
  weighted_sum_ls<-list()
  
  for(i in 1: length(brick_data_annual)){ # for each size bin
    
    # multiply area with values
    x <- brick_data_annual[[i]] * w
    # plot(x)
    # plot(w)

    # compute grid cell weighed sums 
    weighted_sum<-cellStats(x, sum, na.rm = TRUE) 
    weighted_sum_ls[[i]]<-data.frame(Year = unique(indices2), weighted_sum = weighted_sum) %>%
      mutate(
        Year = as.numeric(Year),
        file = netcdf,
        mem = model,
        esm = esm,
        scenario = scenario,
        bin = bins[[i]]) 

    rownames(weighted_sum_ls[[i]])<-NULL

  }

  weighted_sum_df<-do.call(rbind, weighted_sum_ls)
  
  # STEP 3 sum biomass across common bins 

  weighted_sum_allBio<-weighted_sum_df %>%
    filter(bin %in% c(2:5)) %>% # BOATS is the minimum common denominator 10g to 100kg
    group_by(Year, mem, esm, scenario, file) %>%
    summarise(weighted_sum_allBio = sum(weighted_sum)) %>%
    ungroup()
  
  return(list(
    brick_data_annual = brick_data_annual, 
    weighted_sum_df = weighted_sum_df,
    weighted_sum_allBio = weighted_sum_allBio))
  
  rm(brick_data, brick_data_subset, indices, indices2, w, x, weighted_sum, brick_data_annual, weighted_sum_df, weighted_mean_allBio)
  
#   # STEP 2 ALTERNATIVE - calculate annual means
#   # https://gis.stackexchange.com/questions/257090/calculating-and-displaying-mean-annual-precipitation-from-cru-data
#   # create vector to serve as index
#   
#   if(scenario %in% c("historical","picontrol_hist", "picontrol_whole")){ 
#     indices2<-as.Date(indices_subset)
#   }else if(scenario %in% c("ssp1", "ssp5", "picontrol_fut")){
#     indices2<-as.Date(t)}
#   
#   indices2<-format(indices2, format = "%Y")
#   indices2<-as.numeric(indices2)
#   brick_data_annual<-lapply(brick_data_subset, FUN = function(x) stackApply(x, indices=indices2, fun=mean))
# 
#   # # CHECK 
#   # dim(brick_data_annual[[1]])
#   # plot(brick_data_annual[[1]][[15]])
#   
#   if(scenario == "picontrol_whole"){scenario = "picontrol_fut"}
# 
#   # STEP 3 - extract global weighted trends 
#   weighted_mean_lat_ls<-list()
#   # weighted_mean_area_ls<-list()
#   
#   for(i in 1: length(brick_data_annual)){ # for each size bin
#     
#     # i =1
#     # mean values - weighted by grid cell latitude 
#     # https://stackoverflow.com/questions/55230510/calculating-weighted-spatial-global-annual-averages-across-grid-cells-using-netc
#     
#     # raster with latitude cell values 
#     w <- init(brick_data_annual[[i]], 'y')
#     # cosine after transforming to radians
#     w <- cos(w  * (pi/180)) # WARNING - what happens with negative lats? should is be abs(w)? min and max values look the same when using w or abs(w) ...
#     # plot(w)
#     # multiply weights with values
#     x <- brick_data_annual[[i]] * w
#     # plot(x)
#     # plot(w)
#     
#     # remove land in weights too 
#     w2<-mask(w, x, updatevalue=NA)
#     # plot(w2)
#     
#     # compute weighted average 
#     weighted_mean_lat<-cellStats(x, sum, na.rm = TRUE) / cellStats(w2, sum, na.rm = TRUE)
#     weighted_mean_lat_ls[[i]]<-data.frame(Year = unique(indices2), weighted_mean = weighted_mean_lat) %>% 
#       mutate(
#         Year = as.numeric(Year), 
#         file = netcdf, 
#         mem = model,
#         esm = esm,
#         scenario = scenario,
#         bin = bins[[i]]) 
#     rownames(weighted_mean_lat_ls[[i]])<-NULL
#     
#     # # WARNING try with area just to see if results are slightly different 
#     # a <- area(brick_data_annual[[i]]) / 10000 # WARNING above this is *1e6
#     # x <- brick_data_annual[[i]] * a
#     # a2<-mask(a, x, updatevalue=NA)
#     # weighted_mean_area<-cellStats(x, sum, na.rm = TRUE) / cellStats(a2, sum, na.rm = TRUE)
#     # weighted_mean_area_ls[[i]]<-data.frame(Year = unique(indices2), weighted_mean = weighted_mean_area) %>% 
#     #   mutate(
#     #     Year = as.numeric(Year), 
#     #     file = netcdf, 
#     #     mem = model,
#     #     esm = esm,
#     #     scenario = scenario,
#     #     bin = bins[[i]]) 
#     # rownames(weighted_mean_area_ls[[i]])<-NULL
#     
#   }
#   
#   weighted_mean_lat_df<-do.call(rbind, weighted_mean_lat_ls) 
#   # weighted_mean_area_df<-do.call(rbind, weighted_mean_area_ls)
#  
#   ######### sum biomass across common bins ---- 
#   
#   weighted_mean_allBio<-weighted_mean_lat_df %>% 
#     filter(bin %in% c(2:5)) %>% # BOATS is the minimum common denominator 10g to 100kg
#     group_by(Year, mem, esm, scenario, file) %>% 
#     summarise(weighted_mean_allBio = sum(weighted_mean)) %>% 
#     ungroup()
# 
# return(list(brick_data_annual = brick_data_annual, 
#             weighted_mean_lat_df = weighted_mean_lat_df, 
#             weighted_mean_allBio = weighted_mean_allBio))
  } # end of if file exists 
  
} # end of function  


# test function 
test<-extract_global_outputs(netcdf[4]) 

# check
test$brick_data_annual
plot(test$brick_data_annual[[5]][[10]]) # size bin 5, random time step

rm(test)

# now do all:
all_bricks<-pblapply(netcdf,extract_global_outputs,cl=detectCores()-2)

# # CN version just as test - can go back to above : 
# all_bricks<-mclapply(netcdf, function(x) extract_global_outputs(x, file = "new"), mc.cores = detectCores()-2)
# 

# extract data from function object
length(all_bricks)
names(all_bricks) <- combinations$identifier

# annual maps as raster objects
brick_data_annual<-sapply(all_bricks, function(x) x[["brick_data_annual"]])
brick_data_annual[sapply(brick_data_annual, is.null)] <- NULL # remove empty objects
# check
names(brick_data_annual)
plot(brick_data_annual$boats_gfdl_historical[[2]][[1]])

# NOTE: if you need to consider only hist (or ssp126 or ssp585):
hist<-names(brick_data_annual) %>% str_subset("historical")
y_hist<-brick_data_annual[hist]

# annual trend in biomass at size bins level, for each MEM, ESM and scenario combination
weighted_sum_df<-sapply(all_bricks, function(x) x[["weighted_sum_df"]])
weighted_sum_df[sapply(weighted_sum_df, is.null)] <- NULL
# check
weighted_sum_df$boats_gfdl_historical

# annual trend in total biomass (sum across size bins 2 to 5), for each MEM, ESM and scenario combination
weighted_sum_allBio<-sapply(all_bricks, function(x) x[["weighted_sum_allBio"]])
weighted_sum_allBio[sapply(weighted_sum_allBio, is.null)] <- NULL
# check
weighted_sum_allBio$boats_gfdl_historical

## NEED TO CHECK WHICH LAYERS ARE PRESENT ACROSS ALL MODELS

# CN all models include 6 bins, boats include 2-5 (with 1 and 6 being present but empty) and macroecological includes bins 1 to 5. Sums across layers are calcualtred using bins 2 to 5, minimum common denominator. can check bins for each model in weighted_mean_lat_df.

```

### save data as temp_file 

```{r}

save(brick_data_annual,
     weighted_sum_df,
     weighted_sum_allBio,
     file = "/home/ubuntu/climate_mitigation/Tmp_data/Extract_global_MEMs_sums.RData")

```

### check outputs 

```{r}

## NOTE should probably do this to all df before saving 
weighted_sum_df_toplot<- do.call(rbind, weighted_sum_df)
unique(weighted_sum_df_toplot$esm)  

# consider one combination only - not as per Derek's plot!!!!!!! why? load data saved by Julia and explore... 
weighted_sum_df_toplot2<-weighted_sum_df_toplot %>% 
  filter(mem == "apecosm", esm == "ipsl-cm6a-lr")

# macroecological missing future files - to download as for slope paper you are not considering this model adn for SO you are not considering the future
weighted_sum_df_toplot2<-weighted_sum_df_toplot %>% 
  filter(mem == "macroecological", esm == "gfdl-esm4")

ggplot(weighted_sum_df_toplot2, aes(x = Year, y = weighted_sum, group = scenario, color = scenario))+
  geom_line()+
  facet_wrap(~bin, scale = "free")

## all together - DBPM problem - why? i thought abundance even for the smallest size class was lower than for the other models... to check better 
weighted_sum_df_toplot3<- weighted_sum_df_toplot %>% 
  mutate(ID = paste(esm, bin, scenario, sep ="_"))

ggplot(weighted_sum_df_toplot3, aes(x = Year, y = weighted_sum, group = ID, color = scenario))+
  geom_line()+
  facet_wrap(~mem)

## plot maps of values 
plot(brick_data_annual$dbpm_ipsl_historical[[1]][[1]])
plot(brick_data_annual$apecosm_ipsl_historical[[1]][[1]])
plot(brick_data_annual$boats_ipsl_historical[[1]][[1]]) # this is not good as it should be NAs and not 0s
weighted_sum_df$boats_gfdl_historical # these are 0s in df too ... check if this is becasue of the way you multiplied the grid cell area ...  

```

### Julia an Derek version on a function above that 

```{r}
#multiply each grid cell value by cell area and sum over all grid cells.
#to multiply by area,  need to convert area from km2 to m2 (*1e6), and sum over all cells each month.

# # JB version

bricktotime <-function(br=1,all_bricks,lyr=1,month=1){
  monthend<-dim(all_bricks[[br]][[lyr]])[3]
  lyrend<-length(lengths(all_bricks[[br]]))
  bricksum<-matrix(NA,nrow=monthend,ncol=lyrend)
  for(elyr in lyr:lyrend){
      bricksum[,elyr]<-cellStats(all_bricks[[br]][[elyr]][[month:monthend]]*area(all_bricks[[br]][[elyr]][[month:monthend]])*1e6,sum)
  }
  return(bricksum)
}

whichones<-rep(NA,36)
for (i in 1:36) whichones[i]<-length(all_bricks[[i]])
bricklist<-which(whichones>0)

# mylist <- all_bricks %>% 
# purrr::map(`[`,bricklist) 

non_empties<-Filter(Negate(is.null),all_bricks)

#all_bricksums<-list(1:length(non_empties))

# for (i in 1:2){
#  all_bricksums[[i]]<-bricktotime(i,all_bricks=non_empties)
#  }

all_bricksums<-pblapply(1:length(non_empties),bricktotime,all_bricks=non_empties,cl=26)

filenames<-netcdf[bricklist] 

saveRDS(all_bricksums,"TotalBiomassinSizeBinsbyModelRun.RDS")

saveRDS(filenames,"FilenamesTotalBiomassinSizeBinsbyModelRun.RDS")
 

#-------------------------------------------------------------------------------
## DT version 

## CN trial 
rm(list=ls())
# trial # CN using RDS from Julia
mem<-c("apecosm","boats", "dbpm", "zoomss", "ecotroph", "macroecological") 
esm<-c("gfdl-esm4", "ipsl-cm6a-lr")
scenario<-c("historical", "ssp126", "ssp585") 
TotalBiomassinSizeBinsbyModelRun<-readRDS("TotalBiomassinSizeBinsbyModelRun.RDS")
FilenamesTotalBiomassinSizeBinsbyModelRun<-readRDS("FilenamesTotalBiomassinSizeBinsbyModelRun.RDS")

# Save the data in the format required for the project

# Extract the appropriate size bins
# For size-structure models these should be: 1-10g, 10-100g, 100g-1kg, 1-10kg, 10-100kg, >100kg
# But note that models may have different bins

# Mem_bins should contain the number of size bins provided by each model
# AT PRESENT THIS IS JUST DUMMY/TEST DATA
mem_bins = list(apecosm = 6, boats = 6, dbpm = 6, zoomss = 5, ecotroph = 6, macroecological = 5)

# Mem_bins_to_use indicates the size bins that represent 10g - 100kg for each model
# AT PRESENT THIS IS JUST DUMMY/TEST DATA
mem_bins_to_use = list(apecosm = 1:6, boats = 2:5, dbpm = 1:6, zoomss = 1:6, ecotroph = 1:6, macroecological = 1:5)

if (!identical(mem, names(mem_bins)))
  print("Your mem names and list of bins for each mem do not match")

if (!identical(mem, names(mem_bins_to_use)))
  print("Your mem names and list of bins for each mem do not match")

# Set the reference period for each model
reference_period_min_year = 1995
reference_period_max_year = 2014

# Data frame to hold model size-summed outputs
size_summed_outputs = data.frame("climate_model" = NA, "impact_model"= NA, "forcing"= NA, "time"= NA, "biomass"= NA)

data_frame_start = 1
for (ii in 1:length(mem))
{
  # ii = 1
  for (jj in 1:length(esm))
  {
    # jj = 2
    for (kk in 1:length(scenario))
    {
      # kk = 1
      
      # Identify the specific model run
      mem_model = which(FilenamesTotalBiomassinSizeBinsbyModelRun %in% str_subset(FilenamesTotalBiomassinSizeBinsbyModelRun, mem[ii]))
      esm_model = which(FilenamesTotalBiomassinSizeBinsbyModelRun %in% str_subset(FilenamesTotalBiomassinSizeBinsbyModelRun, esm[jj]))
      scenario_run = which(FilenamesTotalBiomassinSizeBinsbyModelRun %in% str_subset(FilenamesTotalBiomassinSizeBinsbyModelRun, scenario[kk]))
      
      filename_to_use = Reduce(intersect, list(mem_model, esm_model, scenario_run))
      if ((length(filename_to_use) == 0) || (length(filename_to_use) >1))
      {
        print("Error when selecting file: either this model/esm/scenario combination was not used, or there are more than one files that match")
        print(paste(mem[ii], esm[jj], scenario[kk], sep = " "))
      } else
      {
        # Extract the data in the appropriate size bin(s)
        min_years = c("1850","1950","2015")
        max_years = c("2014","2100")
        min_year = min(as.numeric(min_years[which(sapply(min_years, grepl, FilenamesTotalBiomassinSizeBinsbyModelRun[filename_to_use]) == TRUE)]))
        max_year = max(as.numeric(max_years[which(sapply(max_years, grepl, FilenamesTotalBiomassinSizeBinsbyModelRun[filename_to_use]) == TRUE)]))
        
        print(paste(mem[ii], esm[jj], scenario[kk], sep = " "))
             
        total_bins_this_model = eval(parse(text = paste("mem_bins$",mem[ii],sep="")))
        bins_to_use_this_model = eval(parse(text = paste("mem_bins_to_use$",mem[ii],sep="")))
        
        print(paste("Total size bins for this model: ", total_bins_this_model))
        print("Bins to use for this model:")
        print(bins_to_use_this_model)
        
        for (ll in data_frame_start:(data_frame_start + (max_year - min_year)))
        {
          size_summed_outputs[ll,1] = esm[jj]
          size_summed_outputs[ll,2] = mem[ii]
          size_summed_outputs[ll,3] = scenario[kk]
          size_summed_outputs[ll,4] = min_year + ll - 1
          size_summed_outputs[ll,5] = sum(TotalBiomassinSizeBinsbyModelRun[[filename_to_use]][(ll - 1) * total_bins_this_model + (1:total_bins_this_model)][bins_to_use_this_model])
        }
        data_frame_start = ll + 1
      }
    }
  }
}


# Now need to put into the format for the marginal impact project; see the pdf that they sent. Needs to be a damage function that is compared to a baseline period

```


