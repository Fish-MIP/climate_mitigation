---
title: "Extract data for climate mitigation project"
author: "Julia Blanchard, Camilla Novaglio, Derek Tittensor"
date: "2023-05-18"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Aim

This script is for extracting FishMIP global marine ecosystem model outputs for use in a cross-sectoral climate mitigation paper being led by Christoph Semken.

For this project we need th global time series of biomass outputs, spanning size ranges typical of exploited fishes - 10g to 1000 kg.

We will calculate annual time series spanning 1850-2100 under the rcp8.5 and rcp2.6 scenarios for each of the models, for whihc outputs are available:

APECOSM  - available, IPSL only
BOATS  - available
DBEM  - possible later to request OR possible using Lmax classes?
DBPM - available
EcoOcean  - possible using Lmax classes?
EcoTroph  - available?
FEISTY  - possible using Lmax classes?
MACROECOLOGICAL  - available
ZooMSS - available

## File dowload 

check FishMIP/EmergentConstraints repo R/01_extract_modelled_data.Rmd for details and notes. 

## Set environment 

```{r cars}

rm(list=ls())

### Read in files and extract modelled data at global scale
library(raster)
library(ncdf4)
library(tidyverse)
library(ncdf4.helpers)
library(parallel)
library(pbapply)
library(dplyr, warn.conflicts = FALSE)

```

## Read in files and extract modelled data at global scale

```{r}

### using files and code prepared by Cami:

dir<-"/rd/gem/private/users/camillan/EmergentConstraintData"

mem<-c("apecosm","boats", "dbpm", "zoomss", "ecotroph", "macroecological") 
esm<-c("gfdl-esm4", "ipsl-cm6a-lr")
scenario<-c("historical", "ssp126", "ssp585") 

# all

combinations<-expand.grid(mem = mem, esm = esm, scenario = scenario) %>%
  mutate(resolution = ifelse(mem %in% c("zoomss", "ecotroph", "macroecological"), "annual", "monthly"), 
         year = case_when(
           mem %in% c("boats", "zoomss", "ecotroph", "macroecological") & scenario == "historical" ~ "1950_2014", 
           mem %in% c("apecosm", "dbpm") & scenario == "historical" ~ "1850_2014",
           scenario %in% c("ssp126", "ssp585") ~ "2015_2100"), 
         mem = as.character(mem), 
         esm = as.character(esm), 
         scenario = as.character(scenario),
         netcdf_name = paste0(paste(mem, esm, "nobasd", scenario, "nat_default_tcblog10_global", resolution, year, sep ="_"), ".nc"),
         esm_simpler = ifelse(esm == "ipsl-cm6a-lr", "ipsl","gfdl"),
         identifier = paste(mem, esm_simpler, scenario, sep ="_")) %>% 
  select(-esm_simpler) %>% 
  arrange(mem, esm, scenario)

# apply function in // ----

netcdf = combinations$netcdf_name

# function to be applied
extract_global_outputs<-function(netcdf,file){
  
  # # trial
  # netcdf = "ecotroph_gfdl-esm4_nobasd_historical_nat_default_tcblog10_global_annual_1950_2014.nc"
  # netcdf = "apecosm_ipsl-cm6a-lr_nobasd_historical_nat_default_tcblog10_global_monthly_1850_2014.nc"
  # 
  # file = "new"

  if(file.exists(file.path(dir, netcdf))){
  
  ######### extract info from netcdf name and print warnings ----
  model = sub("\\_.*", "", netcdf)
  
  if(str_detect(netcdf, "gfdl", negate = FALSE)){
    esm = "gfdl-esm4"
  }else if (str_detect(netcdf, "ipsl", negate = FALSE)){
    esm = "ipsl-cm6a-lr"
  }
  
  # WARNING - add in EC? 
  if(str_detect(netcdf, "monthly", negate = FALSE)){
    time_step = "monthly"
  }else if (str_detect(netcdf, "annual", negate = FALSE)){
    time_step = "annual"
  }
  
  if(str_detect(netcdf, "historical", negate = FALSE)){
    scenario = "historical"
  }else if (str_detect(netcdf, "ssp126", negate = FALSE)){
    scenario = "ssp1"
  }else if (str_detect(netcdf, "ssp585", negate = FALSE)){
    scenario = "ssp5"
  }else if (str_detect(netcdf, "picontrol|2100", negate = FALSE)) {
    scenario = "picontrol_fut"
  } else if (str_detect(netcdf, "picontrol|2014", negate = FALSE)) {
    scenario = "picontrol_hist"}
  
  # extract info from netcdf description: 
  nc_data <- nc_open(file.path(dir, netcdf))
  
  lon <- ncvar_get(nc_data, "lon")
  lat <- ncvar_get(nc_data, "lat", verbose = F)
  t <- as.character(nc.get.time.series(nc_data))
  
  # this is only to FIX zoom size bins names 
  if(model != "zoomss" & file == "new"){
    bins<-ncvar_get(nc_data, "bins")
  }else if (model != "zoomss" & file == "old"){ # this is only to check DBPM old files (not in DKRZ)
    bins<-ncvar_get(nc_data, "size")
  }else if (model == "zoomss"){
    bins<-c(1:6)}
  
  t_units<-ncatt_get(nc_data, "time", "units")$value
  b_units<-ncatt_get(nc_data, "tcblog10", "units")$value
  
  nc_close(nc_data)
  
  # print warnings 
  
  stLon<-lon[1]
  enLon<-lon[length(lon)]
  stLat<-lat[1]
  enLat<-lat[length(lat)]
  stTime<-t[1]
  enTime<-t[length(t)]
  
  if(stLon != -179.5){
    warning(paste(model, esm, scenario, "incorrect starting Lon", sep = " "), immediate. = TRUE)
  }
  if(enLon != 179.5){
    warning(paste(model, esm, scenario, "incorrect ending Lon", sep = " "), immediate. = TRUE)
  }
  if(stLat != 89.5){
    warning(paste(model, esm, scenario, "incorrect starting Lat", sep = " "), immediate. = TRUE)
  }
  if(enLat != -89.5){
    warning(paste(model, esm, scenario, "incorrect ending Lat", sep = " "), immediate. = TRUE)
  }
  if(scenario == "historical" & !stTime %in% c("1950-01-01","1850-01-01")){ # some model include 100 years more 
    warning(paste(model, esm, scenario, "incorrect starting time", sep = " "), immediate. = TRUE)
  }
  if(scenario != "historical" & !stTime %in% c("2015-01-01")){
    warning(paste(model, esm, scenario, "incorrect starting time", sep = " "), immediate. = TRUE)
  }
  if(scenario == "historical" & !enTime %in% c("2014-12-01", "2014-01-01")){ # models can be monthly or annual 
    warning(paste(model, esm, scenario, "incorrect ending time", sep = " "), immediate. = TRUE)
  }
  if(scenario != "historical" & !enTime %in% c("2100-12-01", "2100-01-01")){ # models can be monthly or annual 
    warning(paste(model, esm, scenario, "incorrect ending time", sep = " "), immediate. = TRUE)
  }
  if(t_units != "days since 1601-1-1 00:00:00"){
    warning(paste(model, esm, scenario, "incorrect time units", sep = " "), immediate. = TRUE)
  }
  if(bins[1] != 1 & bins[6] != 6){
    warning(paste(model, esm, scenario, "incorrect bins names", sep = " "), immediate. = TRUE)
  }
  if(bins[length(bins)] != 6){
    warning(paste(model, esm, scenario, "incorrect bins dimension", sep = " "), immediate. = TRUE)
  }
  if(b_units != "g m-2"){
    warning(paste(model, esm, scenario, "incorrect biomass units", sep = " "), immediate. = TRUE)
  }
 
  # extract data as raster object: 
  brick_data<-list()
 
  for (i in 1:length(bins)){
    brick_data[[i]]<-brick(file.path(dir, netcdf), level = i)
    print(dim(brick_data[[i]]))
  }
  
  #### IF JB and DT approach - function end here and output is:
  # return(brick_data)

  ######### WARINING - remove marginal seas using the land-sea IPSL mask provided by Matthias TO DO ----

  ######### calculate total annual sums OR weighted annual means for trends ----

  # STEP 1 - remove 1850-1950 as not all models have them
  indices<-t

  if(scenario %in% c("historical","picontrol_hist")){
    indices_subset<-indices[indices>="1950-01-01"]
    indices_position<-match(indices_subset,indices)
    brick_data_subset<-lapply(brick_data, FUN = function(x) raster::subset(x, indices_position))
  }else if (scenario %in% c("ssp1","ssp5","picontrol_fut")){
    brick_data_subset<-brick_data
    } else if(scenario == "picontrol_whole"){
    indices_subset<-indices[indices>="2015-01-01"]
    indices_position<-match(indices_subset,indices)
    brick_data_subset<-lapply(brick_data, FUN = function(x) raster::subset(x, indices_position))
  }

  # STEP 2 - calculate annual sums following Julia's approach: 
  # Julia: multiply each grid cell value by cell area and sum over all grid cells.
  # to multiply by area, need to convert area from km2 to m2 (*1e6), and sum over all cells each month.
  # from area() If x is a Raster* object: RasterLayer or RasterBrick. Cell values represent the size of the cell in km2
  
  # create vector to serve as index
  # NOTE: in other versions, where you do weighted mean, you fist average across month and then weighted.mean across grid cells. here you first sum across grid cell in each month and then across months. 

  if(scenario %in% c("historical","picontrol_hist", "picontrol_whole")){
    indices2<-as.Date(indices_subset)
  }else if(scenario %in% c("ssp1", "ssp5", "picontrol_fut")){
    indices2<-as.Date(t)}

  indices2<-format(indices2, format = "%Y")
  indices2<-as.numeric(indices2)
  
  weighted_sum_ls<-list()
  
  for(i in 1: length(brick_data_annual)){ # for each size bin

    # i =1

    # raster with latitude cell values
    w <- area(brick_data_subset[[i]])*1e6 # WARNING - code below does /10000 - why?? adjust in EC even if not used, and in FishingEffort when calculating trends in climate inputs (code 09, again not used but should be corrected) 
    
    # multiply area with values
    x <- brick_data_subset[[i]] * w
    # plot(x)
    # plot(w)

    # compute annual sums - WARNING whether it is monthly or annual outputs? i.e. sum across months and then across lat/long? 
    
    # sum across cells 
    weighted_sum<-cellStats(x, sum, na.rm = TRUE) 
    
    # combine data and sum across months if the model is "monthly".  
    weighted_sum_ls[[i]]<-data.frame(Year = indices2, weighted_sum = weighted_sum) %>%
      mutate(
        Year = as.numeric(Year),
        file = netcdf,
        mem = model,
        esm = esm,
        scenario = scenario,
        bin = bins[[i]]) %>% 
      group_by(Year, file, mem, esm, scenario, bin) %>% 
      summarise(weighted_sum_annual = sum(weighted_sum)) %>% 
      ungroup()
    
    rownames(weighted_sum_ls[[i]])<-NULL

  }

  weighted_sum_df<-do.call(rbind, weighted_sum_ls)
  
  ######### sum biomass across common bins ----

  weighted_mean_allBio<-weighted_sum_df %>%
    filter(bin %in% c(2:5)) %>% # BOATS is the minimum common denominator 10g to 100kg
    group_by(Year, mem, esm, scenario, file) %>%
    summarise(weighted_mean_allBio = sum(weighted_mean)) %>%
    ungroup()

return(list(brick_data_annual = brick_data_annual,
            weighted_sum_df = weighted_sum_df,
            weighted_mean_allBio = weighted_mean_allBio))
  
#   # STEP 2 ALTERNATIVE - calculate annual means
#   # https://gis.stackexchange.com/questions/257090/calculating-and-displaying-mean-annual-precipitation-from-cru-data
#   # create vector to serve as index
#   
#   if(scenario %in% c("historical","picontrol_hist", "picontrol_whole")){ 
#     indices2<-as.Date(indices_subset)
#   }else if(scenario %in% c("ssp1", "ssp5", "picontrol_fut")){
#     indices2<-as.Date(t)}
#   
#   indices2<-format(indices2, format = "%Y")
#   indices2<-as.numeric(indices2)
#   brick_data_annual<-lapply(brick_data_subset, FUN = function(x) stackApply(x, indices=indices2, fun=mean))
# 
#   # # CHECK 
#   # dim(brick_data_annual[[1]])
#   # plot(brick_data_annual[[1]][[15]])
#   
#   if(scenario == "picontrol_whole"){scenario = "picontrol_fut"}
# 
#   # STEP 3 - extract global weighted trends 
#   weighted_mean_lat_ls<-list()
#   # weighted_mean_area_ls<-list()
#   
#   for(i in 1: length(brick_data_annual)){ # for each size bin
#     
#     # i =1
#     # mean values - weighted by grid cell latitude 
#     # https://stackoverflow.com/questions/55230510/calculating-weighted-spatial-global-annual-averages-across-grid-cells-using-netc
#     
#     # raster with latitude cell values 
#     w <- init(brick_data_annual[[i]], 'y')
#     # cosine after transforming to radians
#     w <- cos(w  * (pi/180)) # WARNING - what happens with negative lats? should is be abs(w)? min and max values look the same when using w or abs(w) ...
#     # plot(w)
#     # multiply weights with values
#     x <- brick_data_annual[[i]] * w
#     # plot(x)
#     # plot(w)
#     
#     # remove land in weights too 
#     w2<-mask(w, x, updatevalue=NA)
#     # plot(w2)
#     
#     # compute weighted average 
#     weighted_mean_lat<-cellStats(x, sum, na.rm = TRUE) / cellStats(w2, sum, na.rm = TRUE)
#     weighted_mean_lat_ls[[i]]<-data.frame(Year = unique(indices2), weighted_mean = weighted_mean_lat) %>% 
#       mutate(
#         Year = as.numeric(Year), 
#         file = netcdf, 
#         mem = model,
#         esm = esm,
#         scenario = scenario,
#         bin = bins[[i]]) 
#     rownames(weighted_mean_lat_ls[[i]])<-NULL
#     
#     # # WARNING try with area just to see if results are slightly different 
#     # a <- area(brick_data_annual[[i]]) / 10000 # WARNING above this is *1e6
#     # x <- brick_data_annual[[i]] * a
#     # a2<-mask(a, x, updatevalue=NA)
#     # weighted_mean_area<-cellStats(x, sum, na.rm = TRUE) / cellStats(a2, sum, na.rm = TRUE)
#     # weighted_mean_area_ls[[i]]<-data.frame(Year = unique(indices2), weighted_mean = weighted_mean_area) %>% 
#     #   mutate(
#     #     Year = as.numeric(Year), 
#     #     file = netcdf, 
#     #     mem = model,
#     #     esm = esm,
#     #     scenario = scenario,
#     #     bin = bins[[i]]) 
#     # rownames(weighted_mean_area_ls[[i]])<-NULL
#     
#   }
#   
#   weighted_mean_lat_df<-do.call(rbind, weighted_mean_lat_ls) 
#   # weighted_mean_area_df<-do.call(rbind, weighted_mean_area_ls)
#  
#   ######### sum biomass across common bins ---- 
#   
#   weighted_mean_allBio<-weighted_mean_lat_df %>% 
#     filter(bin %in% c(2:5)) %>% # BOATS is the minimum common denominator 10g to 100kg
#     group_by(Year, mem, esm, scenario, file) %>% 
#     summarise(weighted_mean_allBio = sum(weighted_mean)) %>% 
#     ungroup()
# 
# return(list(brick_data_annual = brick_data_annual, 
#             weighted_mean_lat_df = weighted_mean_lat_df, 
#             weighted_mean_allBio = weighted_mean_allBio))
  } # end of if file exists 
  
} # end of function  






# test<-extract_global_outputs(netcdf[4],dir = dir) # apecosm gfdl (netcdf[1] to 3) does not exist and will be skipped in function below
# 
# # check
# dim(test$brick_data_annual)
# plot(test$brick_data_annual[[5]][[780]]) # size bin 5, last time step 

# now do all:

cores<-detectCores()-2
all_bricks<-pblapply(netcdf,extract_global_outputs,cl=cores)









# # CN version just as test - can go back to above : 
# library(tictoc)
# tic()
# all_bricks<-mclapply(netcdf, function(x) extract_global_outputs(x, file = "new"), mc.cores = detectCores()-2)
# toc() # started 10:10pm 
# 
# # extract data from function object
# names(all_bricks) <- combinations$identifier
# 
# # annual maps as raster objects  
# brick_data_annual<-sapply(all_bricks, function(x) x[["brick_data_annual"]])
# brick_data_annual[sapply(brick_data_annual, is.null)] <- NULL # remove empty objects 
# # check 
# names(brick_data_annual)
# plot(brick_data_annual$boats_gfdl_historical[[2]][[1]]) 
# 
# # NOTE: if you need to consider only hist (or ssp126 or ssp585):
# hist<-names(brick_data_annual) %>% str_subset("historical")
# y_hist<-brick_data_annual[hist]
# 
# # annual trend in biomass at size bins level, for each MEM and scenario combination 
# weighted_mean_lat_df<-sapply(all_bricks, function(x) x[["weighted_mean_lat_df"]])
# weighted_mean_lat_df[sapply(weighted_mean_lat_df, is.null)] <- NULL
# # check 
# weighted_mean_lat_df$boats_gfdl_historical
# 
# # annual trend in total biomass (sum across size bins 2 to 5), for each MEM and scenario combination 
# weighted_mean_allBio<-sapply(all_bricks, function(x) x[["weighted_mean_allBio"]])
# weighted_mean_allBio[sapply(weighted_mean_allBio, is.null)] <- NULL
# # check 
# weighted_mean_allBio$boats_gfdl_historical




#### CN this is now old: 

## CHECK raster object - some combos don't exist: CN yes they will be skipped, empty list objects will need to be removed 
all_bricks[[4]][[1]]

# check map of first time slot
plot(all_bricks[[4]][[1]][[1]])


## NOW GET ANNUAL GLOBAL TIME SERIES FOR EACH MODEL AFTER SUMMMING ACROSS ALL LAYERS

# CN now done within function above

## NEED TO CHECK WHICH LAYERS ARE PRESENT ACROSS ALL MODELS

# CN all models include 6 bins, boats include 2-5 (with 1 and 6 being present but empty) and macroecological includes bins 1 to 5. Sums across layers are calcualtred using bins 2 to 5, minimum common denominator. can check bins for each model in weighted_mean_lat_df.

```

### save data as temp_file 

```{r}

# # CN version 
# save(brick_data_annual,
#      weighted_mean_lat_df, 
#      weighted_mean_allBio, 
#      file = "/home/ubuntu/climate_mitigation/Tmp_data/Extract_global_MEMs.RData")


#multiply each grid cell value by cell area and sum over all grid cells.
#to multiply by area,  need to convert area from km2 to m2 (*1e6), and sum over all cells each month.

# # JB version

bricktotime <-function(br=1,all_bricks,lyr=1,month=1){
  
  # # trial 
  # br = 1
  # lyr=1
  # month = 1
  
  monthend<-dim(all_bricks[[br]][[lyr]])[3]
  lyrend<-length(lengths(all_bricks[[br]]))
  bricksum<-matrix(NA,nrow=monthend,ncol=lyrend)
  for(elyr in lyr:lyrend){
      bricksum[,elyr]<-cellStats(all_bricks[[br]][[elyr]][[month:monthend]]*area(all_bricks[[br]][[elyr]][[month:monthend]])*1e6,sum)
  }
  return(bricksum)
}



whichones<-rep(NA,36)
for (i in 1:36) whichones[i]<-length(all_bricks[[i]])
bricklist<-which(whichones>0)

# mylist <- all_bricks %>% 
# purrr::map(`[`,bricklist) 

non_empties<-Filter(Negate(is.null),all_bricks)

#all_bricksums<-list(1:length(non_empties))

# for (i in 1:2){
#  all_bricksums[[i]]<-bricktotime(i,all_bricks=non_empties)
#  }

all_bricksums<-pblapply(1:length(non_empties),bricktotime,all_bricks=non_empties,cl=26)

filenames<-netcdf[bricklist] 

saveRDS(all_bricksums,"TotalBiomassinSizeBinsbyModelRun.RDS")

saveRDS(filenames,"FilenamesTotalBiomassinSizeBinsbyModelRun.RDS")
 

#-------------------------------------------------------------------------------
## DT version 

## CN trial - not sure which data (mem and esm etc.) is used below. 
rm(list=ls())
# trial # CN using RDS from Julia
mem<-c("apecosm","boats", "dbpm", "zoomss", "ecotroph", "macroecological") 
esm<-c("gfdl-esm4", "ipsl-cm6a-lr")
scenario<-c("historical", "ssp126", "ssp585") 
TotalBiomassinSizeBinsbyModelRun<-readRDS("TotalBiomassinSizeBinsbyModelRun.RDS")
FilenamesTotalBiomassinSizeBinsbyModelRun<-readRDS("FilenamesTotalBiomassinSizeBinsbyModelRun.RDS")

# Save the data in the format required for the project

# Extract the appropriate size bins
# For size-structure models these should be: 1-10g, 10-100g, 100g-1kg, 1-10kg, 10-100kg, >100kg
# But note that models may have different bins

# Mem_bins should contain the number of size bins provided by each model
# AT PRESENT THIS IS JUST DUMMY/TEST DATA
mem_bins = list(apecosm = 6, boats = 6, dbpm = 6, zoomss = 5, ecotroph = 6, macroecological = 5)

# Mem_bins_to_use indicates the size bins that represent 10g - 100kg for each model
# AT PRESENT THIS IS JUST DUMMY/TEST DATA
mem_bins_to_use = list(apecosm = 1:6, boats = 2:5, dbpm = 1:6, zoomss = 1:6, ecotroph = 1:6, macroecological = 1:5)

if (!identical(mem, names(mem_bins)))
  print("Your mem names and list of bins for each mem do not match")

if (!identical(mem, names(mem_bins_to_use)))
  print("Your mem names and list of bins for each mem do not match")

# Set the reference period for each model
reference_period_min_year = 1995
reference_period_max_year = 2014

# Data frame to hold model size-summed outputs
size_summed_outputs = data.frame("climate_model" = NA, "impact_model"= NA, "forcing"= NA, "time"= NA, "biomass"= NA)

data_frame_start = 1
for (ii in 1:length(mem))
{
  # ii = 1
  for (jj in 1:length(esm))
  {
    # jj = 2
    for (kk in 1:length(scenario))
    {
      # kk = 1
      
      # Identify the specific model run
      mem_model = which(FilenamesTotalBiomassinSizeBinsbyModelRun %in% str_subset(FilenamesTotalBiomassinSizeBinsbyModelRun, mem[ii]))
      esm_model = which(FilenamesTotalBiomassinSizeBinsbyModelRun %in% str_subset(FilenamesTotalBiomassinSizeBinsbyModelRun, esm[jj]))
      scenario_run = which(FilenamesTotalBiomassinSizeBinsbyModelRun %in% str_subset(FilenamesTotalBiomassinSizeBinsbyModelRun, scenario[kk]))
      
      filename_to_use = Reduce(intersect, list(mem_model, esm_model, scenario_run))
      if ((length(filename_to_use) == 0) || (length(filename_to_use) >1))
      {
        print("Error when selecting file: either this model/esm/scenario combination was not used, or there are more than one files that match")
        print(paste(mem[ii], esm[jj], scenario[kk], sep = " "))
      } else
      {
        # Extract the data in the appropriate size bin(s)
        min_years = c("1850","1950","2015")
        max_years = c("2014","2100")
        min_year = min(as.numeric(min_years[which(sapply(min_years, grepl, FilenamesTotalBiomassinSizeBinsbyModelRun[filename_to_use]) == TRUE)]))
        max_year = max(as.numeric(max_years[which(sapply(max_years, grepl, FilenamesTotalBiomassinSizeBinsbyModelRun[filename_to_use]) == TRUE)]))
        
        print(paste(mem[ii], esm[jj], scenario[kk], sep = " "))
             
        total_bins_this_model = eval(parse(text = paste("mem_bins$",mem[ii],sep="")))
        bins_to_use_this_model = eval(parse(text = paste("mem_bins_to_use$",mem[ii],sep="")))
        
        print(paste("Total size bins for this model: ", total_bins_this_model))
        print("Bins to use for this model:")
        print(bins_to_use_this_model)
        
        for (ll in data_frame_start:(data_frame_start + (max_year - min_year)))
        {
          size_summed_outputs[ll,1] = esm[jj]
          size_summed_outputs[ll,2] = mem[ii]
          size_summed_outputs[ll,3] = scenario[kk]
          size_summed_outputs[ll,4] = min_year + ll - 1
          size_summed_outputs[ll,5] = sum(TotalBiomassinSizeBinsbyModelRun[[filename_to_use]][(ll - 1) * total_bins_this_model + (1:total_bins_this_model)][bins_to_use_this_model])
        }
        data_frame_start = ll + 1
      }
    }
  }
}


# Now need to put into the format for the marginal impact project; see the pdf that they sent. Needs to be a damage function that is compared to a baseline period

```


